"""
Lektion 7 - Automatisk differentiering och optimering
Assignment: Gradients and optimizers

Instructions:
1. Use PyTorch or JAX
2. Keep examples small and explain with comments
"""

# Task 1: Autodiff basics
# TODO: Define f(x) = x**3 + 2*x
# TODO: Use autodiff to compute df/dx at x=3
# TODO: Compare with the analytic derivative

# Task 2: Optimizer comparison
# TODO: Train a small model (e.g., logistic regression)
# TODO: Compare SGD vs Adam for 20-50 epochs
# TODO: Record final loss and accuracy

# Task 3: Reflection
# TODO: Write 4-6 comment lines about when SGD can be preferable to Adam

print("Done! You computed gradients and compared optimizers.")
